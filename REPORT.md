# 1.1 Сравнение на MNIST (20 баллов)
```
# Сравните производительность на MNIST:
# - Полносвязная сеть (3-4 слоя)
# - Простая CNN (2-3 conv слоя)
# - CNN с Residual Block
# 
# Для каждого варианта:
# - Обучите модель с одинаковыми гиперпараметрами
# - Сравните точность на train и test множествах
# - Измерьте время обучения и инференса
# - Визуализируйте кривые обучения
# - Проанализируйте количество параметров
```

```
# Подготовил данные: загрузил MNIST, добавил нормализацию, создал DataLoader'ы.
# Реализовал модели:
# FCN (3 полносвязных слоя)
# Простая CNN (2 сверточных слоя)
# ResCNN (с residual-блоками)
# Обучил модели
# Визуализировал результаты
```

![график](https://github.com/s1lenter/PracticeHomework4/raw/master/plots/1.1.png)

## Выводы по графикам:
### Для MNIST CNN показывает лучший баланс: Точность: 99%/98.5% (train/test). Время: 2x быстрее ResCNN. Параметры: в 5x меньше FCN
### ResCNN избыточна для MNIST: дает прирост всего 0.2% точности vs CNN и требует в 3x больше времени обучения
### FCN уступает CNN - Хуже обобщает (разрыв train/test) и ниже точность при более быстром обучении
### Графики подтверждают, что для простых задач (типа MNIST) сложные архитектуры часто не дают значимых преимуществ.

# 1.2 Сравнение на CIFAR-10

```
# Сравните производительность на CIFAR-10:
# - Полносвязная сеть (глубокая)
# - CNN с Residual блоками
# - CNN с регуляризацией и Residual блоками
# 
# Для каждого варианта:
# - Обучите модель с одинаковыми гиперпараметрами
# - Сравните точность и время обучения
# - Проанализируйте переобучение
# - Визуализируйте confusion matrix
# - Исследуйте градиенты (gradient flow)
```

```
# Загрузил CIFAR-10 с аугментацией (RandomHorizontalFlip, RandomCrop)
# Добавил нормализацию по каналам
# DeepFCN: 5 полносвязных слоев (1024-512-256-128-10) с Dropout=0.5
# ResCNN: Residual архитектура с 3 блоками (64-128-256 каналов)
# Реализовал функцию train_model() с расчетом метрик
# Добавил анализ confusion matrix и gradient flow
# Построил графики accuracy/loss для всех моделей
# Визуализировал confusion matrices
```
![график](https://github.com/s1lenter/PracticeHomework4/raw/master/plots/1.2 graphs.png)
### ResCNN и RegResCNN показывают наилучшие результаты для CIFAR-10.
### Residual-архитектуры стабильнее и быстрее минимизируют потери.
![график](https://github.com/s1lenter/PracticeHomework4/raw/master/plots/1.2 gradient flow.png)
### DeepFCN градиенты быстро затухают. ResCNN/RegResCNN градиенты стабильны, так как Residual-связи предотвращают затухание. RegResCNN показывает чуть больший разброс из-за dropout. Residual-блоки решают проблему затухающих градиентов.

![график](https://github.com/s1lenter/PracticeHomework4/raw/master/plots/1.2 matrix.png)


# На основе проведенного эксперимента можно сделать следующие выводы:
1. Сверточные сети (CNN и ResCNN) показали значительно лучшую точность по сравнению с полносвязной сетью на тестовом наборе.
2. ResCNN достигла наивысшей точности, демонстрируя преимущество residual-блоков.
3. FCN обучалась быстрее всего благодаря простой архитектуре.
4. ResCNN потребовала больше времени на обучение из-за более сложной архитектуры.
5. Все модели показали сравнимое время инференса на батч, с небольшим преимуществом у FCN.
6. FCN имела наибольшее количество параметров, что объясняет ее склонность к переобучению.
7. CNN и ResCNN имели значительно меньше параметров благодаря использованию сверточных слоев.



2.1 Влияние размера ядра свертки

```
# Исследуйте влияние размера ядра свертки:
# - 3x3 ядра
# - 5x5 ядра
# - 7x7 ядра
# - Комбинация разных размеров (1x1 + 3x3)
# 
# Для каждого варианта:
# - Поддерживайте одинаковое количество параметров
# - Сравните точность и время обучения
# - Проанализируйте рецептивные поля
# - Визуализируйте активации первого слоя
```

```
# Реализовал 4 архитектуры с разными ядрами (3x3, 5x5, 7x7, 1x1+3x3)
# Подобрал количество фильтров для равенства параметров (~500K)
# Использовал единые гиперпараметры (Adam, lr=0.001, 30 эпох)
# Фиксировал время эпохи и точность на валидации
```

![график](https://github.com/s1lenter/PracticeHomework4/raw/master/plots/2.1 graphs.png)
Выводы по графикам точности:
Анализ кривых точности четко показывает, что компактные ядра 3×3 демонстрируют наилучший баланс между скоростью обучения и итоговой точностью, достигая 75% на тестовых данных. Комбинация 1×1+3×3 немного уступает в абсолютных значениях (~74%), но обеспечивает более плавную сходимость и меньший разрыв между обучающей и тестовой выборками, что указывает на улучшенную обобщающую способность. Ядра 5×5 и особенно 7×7 не только снижают точность на 3–7%, но и требуют больше эпох для стабилизации, а их кривые обучения проявляют признаки переобучения — резкие колебания и увеличение разрыва между train и test accuracy. Это подтверждает, что для небольших изображений CIFAR-10 избыточное увеличение рецептивного поля вредно.

Выводы по графикам активаций и рецептивных полей:
Визуализация активаций первого слоя раскрывает ключевую разницу: ядра 3×3 и комбинация 1×1+3×3 фокусируются на локальных деталях (границы, текстуры), что критично для распознавания мелких объектов CIFAR-10. В то же время ядра 5×5 и 7×7 дают размытые активации, теряя важные признаки, что объясняет их низкую точность. Рецептивные поля для 3×3 (15px) и 1×1+3×3 (15px) оптимальны для размера изображений 32×32, тогда как 5×5 (23px) и 7×7 (31px) захватывают избыточный контекст, включая шумовые области. Таким образом, эффективность малых ядер обусловлена их способностью выделять релевантные признаки без перегруженности параметрами, что особенно важно для датасетов с низким разрешением.

![график](https://github.com/s1lenter/PracticeHomework4/raw/master/plots/2.1 layers.png)



2.2 Влияние глубины CNN (15 баллов)

```
# Исследуйте влияние глубины CNN:
# - Неглубокая CNN (2 conv слоя)
# - Средняя CNN (4 conv слоя)
# - Глубокая CNN (6+ conv слоев)
# - CNN с Residual связями
# 
# Для каждого варианта:
# - Сравните точность и время обучения
# - Проанализируйте vanishing/exploding gradients
# - Исследуйте эффективность Residual связей
# - Визуализируйте feature maps
```

```
# Реализовал четыре архитектуры CNN с разной глубиной (2, 4, 6+ слоёв и с Residual-связями) на датасете CIFAR-10. 
# Обучил модели с одинаковыми гиперпараметрами (Adam, lr=0.001, batch_size=128) в течение 30 эпох. В процессе обучения фиксировались метрики точности и времени выполнения, анализировались градиенты для выявления проблем vanishing/exploding, а также визуализировались feature maps из разных слоёв.
```

![график](https://github.com/s1lenter/PracticeHomework4/raw/master/plots/2.2.png)

Анализ графиков показывает четкую зависимость между глубиной сети и её производительностью. На графике точности (Test Accuracy) видно, что ResCNN демонстрирует наилучший результат - около 80% к 8-й эпохе, что на 5-10% выше, чем у других архитектур. MediumCNN и ShallowCNN показывают схожие результаты (70-75%), в то время как DeepCNN без Residual-связей отстает, достигая лишь 60-65% точности, что свидетельствует о проблемах с обучением в глубоких слоях.

График градиентов (Gradient Flow) подтверждает эти наблюдения: ResCNN поддерживает стабильно высокий уровень градиентов (около 4×10⁰), тогда как у DeepCNN наблюдается резкий спад градиентов до 2×10⁰ уже к 4-й эпохе, что указывает на проблему затухающих градиентов. MediumCNN и ShallowCNN показывают промежуточные значения, но без резких колебаний. Разница в поведении градиентов объясняет преимущество Residual-связей - они позволяют сохранять устойчивый поток градиентов даже в глубоких сетях, что напрямую влияет на качество обучения. Визуально это проявляется в более быстрой и стабильной сходимости ResCNN по сравнению с другими архитектурами.